{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66e84e9b",
   "metadata": {},
   "source": [
    "# My Mr Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c896befe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import mylib as lib\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a36747e",
   "metadata": {},
   "source": [
    "## 1. Get content from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "131ed81e-61d7-4871-908c-ffded6bda3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(title):\n",
    "    url = f\"https://en.wikipedia.org/w/api.php?action=query&prop=extracts&titles={title}&explaintext&format=json\"  \n",
    "    \n",
    "    respons = requests.get(url)\n",
    "    data = respons.json()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "277aeec7-ddc0-4caa-9862-3b18c7c5a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_content(\"Ozone_layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e88e5b93-d814-4aae-a80f-c6126f042b60",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d5a654",
   "metadata": {},
   "source": [
    "## 2. merge contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdc1cafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_ab(abdict, data):\n",
    "    for key, value in abdict.items():\n",
    "        # print(key)\n",
    "        # print(value)\n",
    "        matche = f\"{key}.*{value}\"\n",
    "        while True:\n",
    "            loc = re.search(matche, data)\n",
    "        \n",
    "            if loc:\n",
    "                a = list(loc.span())[0]\n",
    "                b = list(loc.span())[1]\n",
    "                data = data.replace(data[a:b], \"\")\n",
    "            else:\n",
    "                break     \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c82ed0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_symbol(data):\n",
    "    \n",
    "    while True:\n",
    "        loc = re.search(\" .\\n\", data)\n",
    "        if loc:\n",
    "            a = list(loc.span())[0]\n",
    "            b = list(loc.span())[1]\n",
    "            data = data.replace(data[a:b], \"\")\n",
    "        else:\n",
    "            break\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "743f27df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drops(elements, put, data):\n",
    "    for rep in elements:\n",
    "        data = data.replace(rep, put)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c84bc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_contents(json_data):\n",
    "    key = list(json_data['query']['pages'].keys())[0]\n",
    "    data = str(json_data['query']['pages'][key]['extract'])\n",
    "\n",
    "    data = drop_ab({'{':'}', '\\(' : '\\)'}, data)  \n",
    "\n",
    "    porridge = data.split(\"\\n\\n\\n\")\n",
    "    porridge = porridge[0:-4]\n",
    "\n",
    "    respons = \"\"\n",
    "    \n",
    "    for n in porridge:\n",
    "        n = drop_symbol(n)\n",
    "        n = drops([\"=\", ',', '.', ':', '\"', '-', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0'], \" \", n)\n",
    "        respons += n\n",
    "    respons = drops([\"\\n\", \"\\\\\"], \" \", respons)\n",
    "    return respons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c0651f7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "merge_content = merge_contents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41f68327",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# merge_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c9818c",
   "metadata": {},
   "source": [
    "## 3. Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e1dfe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(content):\n",
    "    content = drops([\",\", \".\"], \" \", content)\n",
    "    data = content.split(\" \")\n",
    "    data2 = []\n",
    "    for n in data:\n",
    "        if len(n) != 0:\n",
    "            data2.append(n)\n",
    "    \n",
    "    return data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e848bf5-767e-40fa-a524-d957945cb637",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = tokenize(merge_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a6e834f-9d6a-4192-8120-02c47a49b456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fefcff",
   "metadata": {},
   "source": [
    "## 4. Lower collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea928613-3c7a-4810-a8be-2d80494c2d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_collection(collection):\n",
    "    respons = []\n",
    "    for n in collection:\n",
    "        respons.append(n.lower())\n",
    "    return respons    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b8d626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_collection = lower_collection(collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b945a7c",
   "metadata": {},
   "source": [
    "## 5. Count frequensy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00b0d258-5cfe-4f5e-9e86-85e0f0d3208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_frequency(collection):\n",
    "    respons = {}\n",
    "    while 0 != len(collection):\n",
    "        word = collection.pop(0)\n",
    "        respons[word] = 1\n",
    "        for n in collection:\n",
    "            if n == word:\n",
    "                collection.remove(n)\n",
    "                respons[word] += 1\n",
    "    return respons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bdba479-46a4-4aed-b740-dc5b28348600",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_collection2 = lower_collection[0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d8c03be-c433-43ee-87b9-2fbe588a1c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 118,\n",
       " 'ozone': 53,\n",
       " 'layer': 13,\n",
       " 'or': 3,\n",
       " 'shield': 1,\n",
       " 'is': 29,\n",
       " 'a': 20,\n",
       " 'region': 1,\n",
       " 'of': 50,\n",
       " \"earth's\": 6,\n",
       " 'stratosphere': 6,\n",
       " 'that': 10,\n",
       " 'absorbs': 3,\n",
       " 'most': 6,\n",
       " \"sun's\": 2,\n",
       " 'ultraviolet': 10,\n",
       " 'radiation': 9,\n",
       " 'it': 8,\n",
       " 'contains': 1,\n",
       " 'high': 5,\n",
       " 'concentration': 3,\n",
       " 'could': 1,\n",
       " 'be': 7,\n",
       " 'used': 3,\n",
       " 'to': 29,\n",
       " 'measure': 2,\n",
       " 'stratospheric': 3,\n",
       " 'from': 5,\n",
       " 'ground': 1,\n",
       " 'between': 5,\n",
       " 'and': 42,\n",
       " 'dobson': 2,\n",
       " 'established': 1,\n",
       " 'worldwide': 3,\n",
       " 'network': 1,\n",
       " 'monitoring': 1,\n",
       " 'stations': 1,\n",
       " 'which': 5,\n",
       " 'continue': 1,\n",
       " 'operate': 1,\n",
       " 'this': 7,\n",
       " 'day': 2,\n",
       " 'unit': 1,\n",
       " 'convenient': 1,\n",
       " 'amount': 2,\n",
       " 'overhead': 1,\n",
       " 'named': 1,\n",
       " 'in': 2,\n",
       " 'his': 1,\n",
       " 'honor': 1,\n",
       " 'percent': 4,\n",
       " 'medium': 1,\n",
       " 'frequency': 1,\n",
       " 'light': 5,\n",
       " 'concerns': 1,\n",
       " 'increased': 3,\n",
       " 'uv': 9,\n",
       " 'due': 3,\n",
       " 'depletion': 6,\n",
       " 'threatened': 1,\n",
       " 'life': 2,\n",
       " 'on': 6,\n",
       " 'earth': 3,\n",
       " 'including': 2,\n",
       " 'skin': 4,\n",
       " 'cancer': 3,\n",
       " 'humans': 1,\n",
       " 'other': 3,\n",
       " 'ecological': 1,\n",
       " 'problems': 1,\n",
       " 'led': 1,\n",
       " 'bans': 2,\n",
       " 'chemicals': 1,\n",
       " 'latest': 1,\n",
       " 'evidence': 1,\n",
       " 'has': 7,\n",
       " 'slowed': 1,\n",
       " 'stopped': 1,\n",
       " 'united': 4,\n",
       " 'nations': 1,\n",
       " 'general': 1,\n",
       " 'assembly': 1,\n",
       " 'designated': 1,\n",
       " 'september': 2,\n",
       " 'as': 8,\n",
       " 'international': 2,\n",
       " 'for': 7,\n",
       " 'preservation': 1,\n",
       " 'venus': 1,\n",
       " 'also': 1,\n",
       " 'thin': 1,\n",
       " 'at': 8,\n",
       " 'an': 5,\n",
       " 'altitude': 1,\n",
       " 'kilometers': 1,\n",
       " 'above': 3,\n",
       " \"planet's\": 1,\n",
       " 'surface': 8,\n",
       " 'sources': 1,\n",
       " 'photochemical': 1,\n",
       " 'mechanisms': 1,\n",
       " 'give': 1,\n",
       " 'rise': 1,\n",
       " 'were': 2,\n",
       " 'discovered': 1,\n",
       " 'by': 11,\n",
       " 'british': 1,\n",
       " 'physicist': 1,\n",
       " 'sydney': 1,\n",
       " 'chapman': 1,\n",
       " 'created': 1,\n",
       " 'striking': 1,\n",
       " 'ordinary': 1,\n",
       " 'oxygen': 5,\n",
       " 'molecules': 2,\n",
       " 'containing': 2,\n",
       " 'two': 1,\n",
       " 'atoms': 1,\n",
       " 'when': 2,\n",
       " 'hits': 1,\n",
       " 'splits': 1,\n",
       " 'into': 5,\n",
       " 'molecule': 1,\n",
       " 'o': 1,\n",
       " 'individual': 1,\n",
       " 'atom': 1,\n",
       " 'continuing': 1,\n",
       " 'process': 1,\n",
       " 'called': 1,\n",
       " 'cycle': 1,\n",
       " 'chemically': 1,\n",
       " 'can': 3,\n",
       " 'described': 2,\n",
       " 'about': 4,\n",
       " 'atmosphere': 7,\n",
       " 'contained': 1,\n",
       " 'concentrations': 2,\n",
       " 'are': 9,\n",
       " 'greatest': 1,\n",
       " 'kilometres': 1,\n",
       " 'thick': 1,\n",
       " 'although': 2,\n",
       " 'very': 3,\n",
       " 'small': 1,\n",
       " 'vitally': 1,\n",
       " 'important': 3,\n",
       " 'because': 2,\n",
       " 'biologically': 1,\n",
       " 'harmful': 3,\n",
       " 'ultravioletuv': 1,\n",
       " 'c': 1,\n",
       " 'all': 2,\n",
       " 'living': 1,\n",
       " 'things': 1,\n",
       " 'entirely': 1,\n",
       " 'screened': 1,\n",
       " 'out': 3,\n",
       " 'combination': 1,\n",
       " 'dioxygen': 1,\n",
       " 'effective': 2,\n",
       " 'screening': 1,\n",
       " 'b;': 1,\n",
       " 'with': 3,\n",
       " 'wavelength': 2,\n",
       " 'nm': 1,\n",
       " 'intensity': 3,\n",
       " 'top': 1,\n",
       " 'million': 1,\n",
       " 'times': 1,\n",
       " 'stronger': 2,\n",
       " 'than': 2,\n",
       " 'nevertheless': 1,\n",
       " 'some': 1,\n",
       " 'b': 1,\n",
       " 'particularly': 1,\n",
       " 'its': 1,\n",
       " 'longest': 1,\n",
       " 'wavelengths': 3,\n",
       " 'reaches': 2,\n",
       " \"skin's\": 1,\n",
       " 'production': 1,\n",
       " 'vitamin': 1,\n",
       " 'd': 1,\n",
       " 'mammals': 1,\n",
       " 'transparent': 1,\n",
       " 'so': 3,\n",
       " 'longer': 1,\n",
       " 'constitutes': 1,\n",
       " 'reaching': 2,\n",
       " 'type': 1,\n",
       " 'significantly': 1,\n",
       " 'less': 1,\n",
       " 'dna': 1,\n",
       " 'may': 3,\n",
       " 'still': 1,\n",
       " 'potentially': 1,\n",
       " 'cause': 2,\n",
       " 'physical': 2,\n",
       " 'damage': 3,\n",
       " 'premature': 1,\n",
       " 'aging': 1,\n",
       " 'indirect': 1,\n",
       " 'genetic': 1,\n",
       " 'distribution': 1,\n",
       " 'thickness': 2,\n",
       " 'varies': 2,\n",
       " 'generally': 1,\n",
       " 'thinner': 2,\n",
       " 'near': 2,\n",
       " 'equator': 1,\n",
       " 'thicker': 1,\n",
       " 'poles': 3,\n",
       " 'refers': 1,\n",
       " 'how': 1,\n",
       " 'much': 3,\n",
       " 'column': 1,\n",
       " 'over': 4,\n",
       " 'given': 1,\n",
       " 'area': 1,\n",
       " 'season': 2,\n",
       " 'reasons': 1,\n",
       " 'these': 4,\n",
       " 'variations': 1,\n",
       " 'atmospheric': 1,\n",
       " 'circulation': 5,\n",
       " 'patterns': 3,\n",
       " 'solar': 2,\n",
       " 'majority': 1,\n",
       " 'produced': 2,\n",
       " 'tropics': 3,\n",
       " 'transported': 1,\n",
       " 'towards': 1,\n",
       " 'wind': 1,\n",
       " 'northern': 5,\n",
       " 'hemisphere': 2,\n",
       " 'known': 2,\n",
       " 'brewer–dobson': 2,\n",
       " 'make': 1,\n",
       " 'thickest': 1,\n",
       " 'spring': 4,\n",
       " 'thinnest': 2,\n",
       " 'fall': 1,\n",
       " 'done': 1,\n",
       " 'lifting': 1,\n",
       " 'poor': 1,\n",
       " 'air': 5,\n",
       " 'troposphere': 1,\n",
       " 'where': 1,\n",
       " 'sun': 1,\n",
       " 'photolyzes': 1,\n",
       " 'turns': 1,\n",
       " 'them': 1,\n",
       " 'then': 1,\n",
       " 'rich': 1,\n",
       " 'carried': 1,\n",
       " 'higher': 3,\n",
       " 'latitudes': 6,\n",
       " 'drops': 1,\n",
       " 'lower': 1,\n",
       " 'layers': 1,\n",
       " 'research': 1,\n",
       " 'found': 2,\n",
       " 'levels': 2,\n",
       " 'states': 3,\n",
       " 'highest': 2,\n",
       " 'months': 3,\n",
       " 'april': 3,\n",
       " 'lowest': 2,\n",
       " 'october': 2,\n",
       " 'while': 1,\n",
       " 'total': 1,\n",
       " 'increases': 1,\n",
       " 'moving': 1,\n",
       " 'greater': 2,\n",
       " 'southern': 2,\n",
       " 'columns': 1,\n",
       " 'occasionally': 1,\n",
       " 'exceeding': 1,\n",
       " 'du': 3,\n",
       " 'averaging': 1,\n",
       " 'whereas': 1,\n",
       " 'constituted': 1,\n",
       " 'usual': 1,\n",
       " 'maximum': 1,\n",
       " 'antarctic': 4,\n",
       " 'before': 1,\n",
       " 'anthropogenic': 1,\n",
       " 'difference': 2,\n",
       " 'occurred': 1,\n",
       " 'naturally': 1,\n",
       " 'weaker': 1,\n",
       " 'polar': 2,\n",
       " 'vortex': 1,\n",
       " 'owing': 1,\n",
       " 'hemisphere’s': 1,\n",
       " 'large': 1,\n",
       " 'mountain': 1,\n",
       " 'ranges': 1,\n",
       " 'contrasts': 1,\n",
       " 'land': 1,\n",
       " 'ocean': 3,\n",
       " 'temperatures': 1,\n",
       " 'since': 2,\n",
       " 's': 3,\n",
       " 'hole': 2,\n",
       " 'phenomenon': 1,\n",
       " 'amounts': 2,\n",
       " 'arctic': 1,\n",
       " 'during': 2,\n",
       " 'march': 1,\n",
       " 'but': 2,\n",
       " 'summer': 1,\n",
       " 'depleted': 1,\n",
       " 'free': 1,\n",
       " 'radical': 1,\n",
       " 'catalysts': 1,\n",
       " 'nitric': 1,\n",
       " 'oxide': 1,\n",
       " 'emitted': 1,\n",
       " 'through': 1,\n",
       " 'human': 1,\n",
       " 'activities': 1,\n",
       " 'breakdown': 1,\n",
       " 'results': 1,\n",
       " 'reduced': 1,\n",
       " 'absorption': 1,\n",
       " 'consequently': 1,\n",
       " 'unabsorbed': 1,\n",
       " 'dangerous': 1,\n",
       " 'able': 3,\n",
       " 'reach': 1,\n",
       " 'have': 5,\n",
       " 'dropped': 1,\n",
       " 'average': 1,\n",
       " 'late': 1,\n",
       " 'approximately': 1,\n",
       " 'around': 1,\n",
       " 'north': 1,\n",
       " 'south': 1,\n",
       " 'larger': 1,\n",
       " 'seasonal': 1,\n",
       " 'declines': 1,\n",
       " 'been': 2,\n",
       " 'seen': 2,\n",
       " 'holes': 2,\n",
       " 'let': 1,\n",
       " 'actually': 1,\n",
       " 'patches': 1,\n",
       " 'parts': 1,\n",
       " 'points': 1,\n",
       " 'axis': 1,\n",
       " 'discovery': 2,\n",
       " 'annual': 1,\n",
       " 'was': 2,\n",
       " 'first': 1,\n",
       " 'announced': 1,\n",
       " 'joe': 1,\n",
       " 'farman': 1,\n",
       " 'brian': 1,\n",
       " 'gardiner': 1,\n",
       " 'jonathan': 1,\n",
       " 'shanklin': 1,\n",
       " 'paper': 1,\n",
       " 'appeared': 1,\n",
       " 'nature': 1,\n",
       " 'regulation': 2,\n",
       " 'attempts': 1,\n",
       " 'included': 1,\n",
       " 'not': 2,\n",
       " 'limited': 1,\n",
       " 'clean': 2,\n",
       " 'act': 3,\n",
       " 'implemented': 1,\n",
       " 'environmental': 1,\n",
       " 'protection': 1,\n",
       " 'agency': 1,\n",
       " 'introduced': 1,\n",
       " 'requirement': 1,\n",
       " 'national': 1,\n",
       " 'ambient': 1,\n",
       " 'quality': 1,\n",
       " 'standards': 1,\n",
       " 'proved': 1,\n",
       " 'quite': 1,\n",
       " 'useful': 1,\n",
       " 'compared': 1,\n",
       " 'global': 1,\n",
       " 'climate': 1,\n",
       " 'change': 1,\n",
       " 'case': 1,\n",
       " 'more': 1,\n",
       " 'hot': 2,\n",
       " 'issue': 1,\n",
       " 'imminent': 1,\n",
       " 'risk': 1,\n",
       " 'lay': 1,\n",
       " 'people': 1,\n",
       " 'cautious': 1,\n",
       " 'risks': 2,\n",
       " 'bad': 1,\n",
       " 'adverse': 1,\n",
       " 'health': 1,\n",
       " 'respiratory': 1,\n",
       " 'effects': 2,\n",
       " 'european': 2,\n",
       " 'union': 1,\n",
       " 'strict': 1,\n",
       " 'guidelines': 1,\n",
       " 'what': 1,\n",
       " 'products': 1,\n",
       " 'allowed': 1,\n",
       " 'bought': 1,\n",
       " 'distributed': 1,\n",
       " 'specific': 1,\n",
       " 'areas': 1,\n",
       " 'expected': 1,\n",
       " 'heal': 1,\n",
       " 'time': 2,\n",
       " 'canada': 1,\n",
       " 'norway': 1,\n",
       " 'enacted': 1,\n",
       " 'cfc': 1,\n",
       " 'aerosol': 1,\n",
       " 'sprays': 1,\n",
       " 'community': 1,\n",
       " 'rejected': 1,\n",
       " 'analogous': 1,\n",
       " 'proposal': 1,\n",
       " 'do': 2,\n",
       " 'same': 1,\n",
       " 'u': 1,\n",
       " 'chlorofluorocarbons': 1,\n",
       " 'continued': 1,\n",
       " 'applications': 1,\n",
       " 'such': 1,\n",
       " 'refrigeration': 1,\n",
       " 'industrial': 1,\n",
       " 'cleaning': 1,\n",
       " 'until': 2,\n",
       " 'after': 2,\n",
       " 'negotiation': 1,\n",
       " 'treaty': 1,\n",
       " 'compounds': 1,\n",
       " 'destroy': 1,\n",
       " 'residual': 1,\n",
       " 'cfcs': 1,\n",
       " 'accumulating': 1,\n",
       " 'within': 1,\n",
       " 'lead': 1,\n",
       " 'gradient': 1,\n",
       " 'organohalogen': 1,\n",
       " 'compound': 1,\n",
       " 'dissolve': 1,\n",
       " \"ocean's\": 1,\n",
       " 'waters': 1,\n",
       " 'dependent': 1,\n",
       " 'tracer': 2,\n",
       " 'helps': 1,\n",
       " 'scientists': 1,\n",
       " 'study': 2,\n",
       " 'tracing': 1,\n",
       " 'biological': 1,\n",
       " 'chemical': 1,\n",
       " 'pathways': 1,\n",
       " 'implications': 1,\n",
       " 'astronomy': 1,\n",
       " 'prevents': 1,\n",
       " 'energetic': 1,\n",
       " 'astronomical': 1,\n",
       " 'data': 1,\n",
       " 'gathered': 1,\n",
       " 'satellites': 1,\n",
       " 'orbiting': 2,\n",
       " 'young': 1,\n",
       " 'stars': 1,\n",
       " 'studying': 1,\n",
       " 'origins': 1,\n",
       " 'galaxies': 1,\n",
       " 'galaxy': 1,\n",
       " 'evolution': 1,\n",
       " 'explorer': 1,\n",
       " 'galex': 1,\n",
       " 'space': 1,\n",
       " 'telescope': 1,\n",
       " 'launched': 1,\n",
       " 'operated': 1}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_frequency(lower_collection2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3d55fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d837d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git add ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd54c142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641fb791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172fdeeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583234c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34762824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c284f704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9fadcee1-1560-41ad-9100-a881a65234f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[94m\n",
      " ██████╗ ██╗    ██╗ █████╗ ███████╗ █████╗ ██████╗    ██╗ ██████╗ \n",
      "██╔═══██╗██║    ██║██╔══██╗██╔════╝██╔══██╗██╔══██╗   ██║██╔═══██╗\n",
      "██║   ██║██║ █╗ ██║███████║███████╗███████║██████╔╝   ██║██║   ██║\n",
      "██║▄▄ ██║██║███╗██║██╔══██║╚════██║██╔══██║██╔══██╗   ██║██║   ██║\n",
      "╚██████╔╝╚███╔███╔╝██║  ██║███████║██║  ██║██║  ██║██╗██║╚██████╔╝\n",
      " ╚══▀▀═╝  ╚══╝╚══╝ ╚═╝  ╚═╝╚══════╝╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝ ╚═════╝ \n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \u001b[0m\n",
      "Booting Gandalf v4.1.12-deab022 (production)\n",
      "Loading parameters:  \u001b[32mOK\u001b[0m\n",
      "User temirov_s connection: \u001b[32mOK\u001b[0m\n",
      "\n",
      "\n",
      "<MY_MR_CLEAN>\n",
      "Directory . exists: \u001b[32mOK\u001b[0m\n",
      "Pushing exercise: \u001b[32mOK\u001b[0m\n",
      "Printing your report:\n",
      "\n",
      " \u001b[97;1;40m     REPORT      \u001b[0m  \u001b[97mMY MR CLEAN\u001b[0m \n",
      "\n",
      " Status             \u001b[92mSUCCESS\u001b[0m     \n",
      " Execution Runtime  1.825175    \n",
      " Score              [\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[33m-\u001b[0m] 4/5 \n",
      "\n",
      "\n",
      "Checks detail Report:\n",
      "\n",
      " \u001b[97;1;40mTEST COUNT FREQUENCY \u001b[0m  \u001b[91m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            FAILURE                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[0m \n",
      "\n",
      " Input                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
      " Expected Output        \"\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
      " Expected Return Value                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
      " Output                     def test_count_frequency(self):                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
      "                                data = ['lorem', 'ipsum', 'dolor', 'sit', 'amet', 'consectetur', 'adipiscing', 'elit', 'aliquam', 'a', 'cursus', 'elit', 'sed', 'viverra', 'justo', 'integer', 'tempus', 'ante', 'at', 'odio', 'commodo', 'et', 'ornare', 'ipsum', 'dapibus', 'donec', 'bibendum', 'sit', 'amet', 'urna', 'sit', 'amet', 'facilisis', 'donec', 'accumsan', 'odio', 'ligula', 'proin', 'sodales', 'sapien', 'at', 'risus', 'viverra', 'at', 'vehicula', 'dolor', 'malesuada', 'aenean', 'scelerisque', 'tristique', 'ultricies', 'lorem', 'ipsum', 'dolor', 'sit', 'amet', 'consectetur', 'adipiscing', 'elit', 'etiam', 'euismod', 'scelerisque', 'lacinia', 'proin', 'sit', 'amet', 'nisi', 'orci', 'donec', 'aliquet', 'eros', 'ac', 'sem', 'fringilla', 'eu', 'rutrum', 'urna', 'malesuada', 'cras', 'ut', 'congue', 'massa', 'nam', 'fermentum', 'odio', 'ac', 'purus', 'varius', 'ac', 'tempus', 'elit', 'bibendum', 'donec', 'turpis', 'leo', 'imperdiet', 'eget', 'blandit', 'non', 'commodo', 'auctor', 'risus', 'praesent', 'interdum', 'bibendum', 'gravida'] \n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
      "                                gold_count_frequency = {'lorem': 2, 'ipsum': 3, 'dolor': 3, 'sit': 5, 'amet': 5, 'consectetur': 2, 'adipiscing': 2, 'elit': 4, 'aliquam': 1, 'a': 1, 'cursus': 1, 'sed': 1, 'viverra': 2, 'justo': 1, 'integer': 1, 'tempus': 2, 'ante': 1, 'at': 3, 'odio': 3, 'commodo': 2, 'et': 1, 'ornare': 1, 'dapibus': 1, 'donec': 4, 'bibendum': 3, 'urna': 2, 'facilisis': 1, 'accumsan': 1, 'ligula': 1, 'proin': 2, 'sodales': 1, 'sapien': 1, 'risus': 2, 'vehicula': 1, 'malesuada': 2, 'aenean': 1, 'scelerisque': 2, 'tristique': 1, 'ultricies': 1, 'etiam': 1, 'euismod': 1, 'lacinia': 1, 'nisi': 1, 'orci': 1, 'aliquet': 1, 'eros': 1, 'ac': 3, 'sem': 1, 'fringilla': 1, 'eu': 1, 'rutrum': 1, 'cras': 1, 'ut': 1, 'congue': 1, 'massa': 1, 'nam': 1, 'fermentum': 1, 'purus': 1, 'varius': 1, 'turpis': 1, 'leo': 1, 'imperdiet': 1, 'eget': 1, 'blandit': 1, 'non': 1, 'auctor': 1, 'praesent': 1, 'interdum': 1, 'gravida': 1}                                                                                                                 \n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
      "                                user_count_frequency = eg.count_frequency(data)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
      "                                self.assertTrue(isinstance(user_count_frequency, dict))                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
      "                                self.assertTrue(gold_count_frequency == user_count_frequency)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
      " Return Value                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
      "\n",
      "Your Stderr:\n",
      "module 'my_mr_clean' has no attribute 'count_frequency'\n",
      "\n",
      " \u001b[97;1;40mTEST GET CONTENT\u001b[0m  \u001b[92mSUCCESS\u001b[0m \n",
      "\n",
      "\n",
      "\n",
      " \u001b[97;1;40mTEST LOWER COLLECTION\u001b[0m  \u001b[92mSUCCESS\u001b[0m \n",
      "\n",
      "\n",
      "\n",
      " \u001b[97;1;40mTEST MERGE CONTENTS\u001b[0m  \u001b[92mSUCCESS\u001b[0m \n",
      "\n",
      "\n",
      "\n",
      " \u001b[97;1;40mTEST TOKENIZE\u001b[0m  \u001b[92mSUCCESS\u001b[0m \n",
      "\n",
      "\n",
      "</MY_MR_CLEAN>\n"
     ]
    }
   ],
   "source": [
    "! gandalf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
